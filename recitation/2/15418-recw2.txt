15-418/618 Spring 2017
Recitation Notes Week 2
27 Jan 2017

I. GETTING STARTED

Prereading in
   "Computer Systems, A Programmer's Perspective, 3rd edition" (CS:APP)

   Required: 5.7 -- 5.10
   Useful:   5.11 -- 5.12

Login to one of the GHC machines (ghc[26-46].ghc.andrew.cmu.edu).

Retrieve a copy of

    /afs/cs.cmu.edu/academic/class/15418-s17/recitations/recw2.tar

Untar it.  You'll see two subdirectories: "recw2-code/conventional"
and "recw2-code/ispc"

UNDERSTANDING YOUR TARGET SYSTEM

Answer the following by studying the file /proc/cpuinfo

1. What is the CPU type
2. How many processors are available?
   a. Number of cores?
   b. Number of threads/core?
3. What is the clock rate?


Now do a web search on the CPU model name.

1. What specs does Intel provide?
   a. When was it made?
   b. What semiconductor technology?  Is this state of the art?
   c. What's the listed price (remember, that's just for the chip).
2. How do the specs match what you've seen from the machine itself?
3. How big is the L3 cache?
4. How much power does it use?  Is that a lot?
5. What is the rated memory bandwidth?
6. What SIMD extension(s) does it support?


Find this CPU on:
   https://en.wikipedia.org/wiki/List_of_Intel_Xeon_microprocessors
(make sure you get the right version.  Check the technology and dates).

What is the Intel code name for the chip's microarchitecture?

Learn about this microarchitecture:
     http://www.agner.org/optimize/microarchitecture.pdf

Take a look at the list of execution units:

1. Consider hardware for supporting floating-point operations:
   a. What are the latencies, capacities and pipeline capabilities for
      +, *, -, and /?
   b. What is FMA?  How can it be used?
   c. What is the support for floating-point vector operations?
2. Consider loads and stores
   a. How many reads can it perform per clock cycle?
   b. How many writes per clock cycle?

Suggested reading:

Compare to Haswell microarchitecture in Section 5.7 of CS:APP

* Note the list of execution units available for the Intel Haswell
  microarchitecture (p. 521).

* Note the description of the latency, issue times, and capacities for
  different operations (p. 523).

II. CONVENTIONAL CODE OPTIMIZATION

You will make changes to the file:

    recw2-code/conventional/sin.cpp

After you make changes, you should then be able to recompile code with
the command "make" and run the code as "./sin".

Let's look at the example presented in class of computing a Taylor's
series approximation of the sin function:

void sinx_reference(int N, int terms, float * x, float *result)
{
    for (int i=0; i<N; i++) {
	float value = x[i];
	float numer = x[i]*x[i]*x[i];
	int denom = 6; // 3!
	int sign = -1;

	for (int j=1; j<=terms; j++) {
	    value += sign * numer / denom;
	    numer *= x[i] * x[i];
	    denom *= (2*j+2) * (2*j+3);
	    sign *= -1;
	}

	result[i] = value;
    }
}

Study this code
  a. What constitutes the inner loop of this function?
  b. What does the inner loop compute?  How many times is it executed,
     expressed in terms of the function arguments?
  c. Given your knowledge of microarchitecture, what part(s) of the inner
     loop look especially problematic?


BENCHMARKING

Run the sin program.

Look at the output and study the benchmarking code in main.cpp.  For
now, don't try to understand the code that implements multithreading.

1. What do the different fields in the output mean?  Focus on the
   measurements for the initial (reference) version.
   a. What was the overall execution time?  Is this a reasonable
      duration for measuring program performance?   Why/why not?
   b. What is the meaning of the terms N, T, r, and t?
   c. Given these parameters, How many executions of the inner loop of
      the function get computed?
   d. How is the value "ns/element" computed?  
   e. About how many clock cycles are required per element in the
      reference version?
2. What happens if you change N, T, or r by 0.5x and by 2x?
   a. Run ./sin -h to see the list of options.
   b. What goes wrong when you try to increase the parameter T?  Why?


Optimizing single-threaded performance.

1. Fill in the function sinx_better by doing some simple optimizations
   to the original code.  Don't do any major restructing, e.g., by
   introducing new loops, and don't do anything exotic.  How much
   improvement can you get?


2. Let's focus on the computation and use of local variable denom.
   a. What part of the Taylor expansion computation does this
      correspond to?
   b. Given your knowledge of the microarchitecture, why will this
      code cause serious performance problems?
   c. What's an important feature of this computation that would allow
      you to restructure and simplify the code?
   d. Fill in the function sinx_predenom to implement this restructuring.
   e. What performance do you get?



3. Let's simplify the computation and use of local variable sign.
   a. What purpose does this value serve?
   b. Can you fold it into your code for handling denom?
   c. Fill in the function sinx_predenoms to implement this version.
   d. What performance do you get?



CODE ANALYSIS

If you got a good implementation of sinx_predenoms, you should have a
fairly tight inner loop.

1. Look at the assembly code in the file sin.s.
   a. C++ compilers generate "mangled" names to deal with templated
      code.  Even though our code is not templated, the names are a bit
      weird, but you can figure out which is the function of interest.
   b. Use your reverse engineering skills to figure isolate those
      instructions forming the inner loop. (Hint: work backward from
      the end of the function.)


2. How many instructions are there?  What does each of them do?
   Either annotate the code, or write a line-by-line C function to
   describe what's going on.  Don't bother looking up instructions.
   Just make educated guesses.


3. How many clock cycles would this loop take per iteration?  (Hint:
   You can use the data-flow method described in Sect. 5.7.3 of CS:APP.)


LOOP UNROLLING

Fill in the function sinx_unrollx2 to implement simple loop unrolling,
computing two terms per iteration.  Your code should work even when
parameter "terms" is not a multiple of the degree of unrolling.  Refer
to CS:APP Section 5.8 if you are unfamiliar with loop unrolling.


Evaluate its performance:
  a. How much does it improve on the previous code?
  b. Why is it faster (or not)?
  c. What limits its performance?  Hint: Use assembly code as a guide.


REASSOCIATION

Assume FP math is distributive and associative.  Rewrite the unrolled
code to reduce the critical path length and reduce the number floating
point operations.  Fill in the function sinx_unroll2xa.


Evaluate its performance:
  a. How much does it improve on the previous code?
  b. Why is it faster (or not)?
  c. What limits its performance?  Hint: Use assembly code as a guide.


It's possible to keep unrolling and reassociating further.

Theoretical Analysis: Imagine that T is very large, and so the effect
of loop overhead is small.
   a. If there were an unbounded number of execution units available,
      how many clock cycles would be required as a function of the
      unrolling factor k.
   b. Given the particular execution units on this processor, how
      would that affect your estimate?



Here's the ns/element measured for our implementations:

2xa: 0.70
3xa: 0.50
4xa: 0.76
5xa: 0.48

a. How do these match your theoretical model?
b. What factors cause the performance to not meet the model?
c. Why are the times not monotonic in k?
d. Do you think the relative performance for different values of k
   would change if there were 16 terms?



III. SIMD EXECUTION

Our optimization target is AVX2.  Read:
    https://en.wikipedia.org/wiki/Advanced_Vector_Extensions

a. What vector length does AVX2 support?
b. How many float's can be held in an AVX vector?
c. How many vector registers are there?


We will use ISPC to do our vectorizing.

Check out code in recw2-code/ispc/sin.ispc.

You see an ISPC rendering of the reference implementation of the sin
computation.  This is very similar to versions that were shown in
class.  

a. Can you detect the difference?


b. How does the regular implementation of sinx_reference differ from the
   one suitable for ISPC vectorization?  What is the reasoning behind it?

Compile and run this code.

a. How much faster is it than the regular code?
b. How does this correspond to your expectation?

In our "better" version, the ISPC code ran in 0.63ns/element, versus
7.16 for the unvectorized code, a speedup of 9.83x.

a. Does this seem possible?
b. What explanations could you give?


Fill in the function sinx_best with your best ISPC code for this problem.


a. How fast does it run?
b. How much speedup does this provide over the unvectorized code?
c. Combining the code optimization and the
   vectorization by ISPC, how much speedup have you gotten over the
   original code?
d. Was the work you did getting this point worth the effort?


IV. THREADING

Conventional code

Returning back to the conventional code, you will see that already
supports multiple threads via pthreads.  Study the support in
main.cpp.

a. How many threads get spawned each time the program is run?
b. How many elements are computed by each thread for each run?
c. How does the code partition the work when the total number elements
   N is not a multiple of the number of threads t?
d. Do you expect this program to run well on multiple cores?  Why?
e. What performance do you see as you increase the number of threads
   beyond 1?  (You can go up to 100).  Is the code faster?  Does it
   scale well with the number of threads?
f. How do you explain these numbers?
g. What happens if you change parameter r to 5000?  Explain.


ISPC tasks

Fill in the function sinx_reference_t to serve as an ISPC task.  It
has an extra parameters "npt" equal to the number of values of x this
thread should process.  It should work correctly even when N is not
divisible by the number of threads.

Fill in the function sinx_reference_task2 to launch 2 tasks in
computing the sin function across the array.

a. How many threads get spawned each time the program is run?
b. How many elements are computed by each thread for each run?
c. In what way is it fair or not fair to compare the performance of
   this multithreaded code to the conventaional, pthtreaded code?
d. What performance do you see as you increase the number of threads
   beyond 1?  (You can go up to 100).  Is the code faster?  Does it
   scale well with the number of threads?
e. How do you explain these numbers?
f. What happens if you change parameter r to 5000?  Explain.


V. REVIEW

The following questions could have answers "true", "false", or
"depends".  Be prepared to justify your answers.


C1. The conventional code would run slower on a machine that does not
   have FMA instructions.

C2. The conventional code would run faster if the SIMD vectors were longer.

C3. The conventional code would run faster on a single core if higher
   levels of hyperthreading were supported. false
   
C4. The branch prediction logic does really well on the different
   versions of the sin function. 

C5. Modern compilers are good at improving code performance by
    eliminating redundant computations.

C6. Modern compilers are good at enhancing the ILP of a program.
False


S1. Most code programmers write never makes use of the SIMD instructions.
True

S2. ISPC can improve single-threaded code performance, even on machines
    that don't have SIMD instructions.
false

S3. ISPC-generated vector code would run around 2x slower on a machine that only
    supports SSE4.

S4. ISPC code needs to be recompiled in moving from a machine with only
    SSE instructions to one that supports AVX2.
True


T1. ISPC tasks require much less effort to spawn than pthread threads.
True

T2. Spawning lots of ISPC tasks can help with load balancing.
True

T3. Even if the speedup is not proportional to the number of threads,
    there's no reason not to spawn lots of threads on a multicore machine.
