15-418/618 Spring 2017
Recitation Notes Week 3
27 Jan 2017

I. GETTING STARTED

Make sure you have done the first part of Assignment 2, where you must
fix up the file saxpy.cu so that it compiles and performs the SAXPY
operation.

Login to one of the GHC machines (ghc[26-46].ghc.andrew.cmu.edu).

Retrieve a copy of

    /afs/cs.cmu.edu/academic/class/15418-s17/recitations/recw3.tar

Untar it.  You'll see the subdirectories: "recw3-code/reduce"

Within that directory, you should be able to run "make".

II. REDUCTIONS

Today will explore some of the nuances of CUDA programming.  As a
programming example, we will perform the simple task of summing the
elements of a floating-point vector, as given by the following code:

float sumVector(int length, float *data) {
    float val = 0.0;
    for (int i = 0; i < length; i++)
	val += data[i];
    return val;
}

1. What would be a good way to do this using a machine with LOTS of
   processing elements?

2. How about with a big SIMD array?
   Show your overall design with an illusration.

III. Simple CUDA Implementation

A CUDA application involves writing code for three different types of
computing elements:

1. The host processor.  Regular C/C++
2. The device's processor.  Regular C/C++
3. The individual thread processors.  Kernels

A "CUDA program" contains the code for #2 & #3.

Here's a kernel for reducing a vector from length to nlength:
// CUDA kernel
// Destructively reduce data from length to nlength.
__global__ void
inplaceReduceKernel(int length, int nlength, float *data) {
    // idx will range between 0 and nlength - 1
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < nlength) {
	float val = data[idx];
	for (int i = idx + nlength; i < length; i += nlength)
	    val += data[i];
	data[idx] = val;
    }
}

Understanding this code:

1. What do blockIdx.x, blockDim.x, and threadIdx.x mean?

2. As the comment indicates, idx will range from 0 to nlength-1

3. Suppose, for example, that length = N and nlength = N/2.
   What will be the effect of this kernel on vector data?

Launching a kernel.

Here's a function that will run on the device controller that performs
sum reduction:

// Reducer based on inplace reduction 
float
inplaceReduce(int length, float *srcVecDevice, float *scratchVecDevice) {
    int threadsPerBlock = reduceThreadsPerBlock;
    int degree = reduceDegree;
    float val;
    int nlength;
    // Copy source data into scratch area so that can modify it
    cudaMemcpy(scratchVecDevice, srcVecDevice, length * sizeof(float),
	       cudaMemcpyDeviceToDevice);
    for (; length > 1; length = nlength) {
    // Make sure you have enough blocks. to round up.
	nlength = UPDIV(length, degree);
	int blocks = UPDIV(nlength, threadsPerBlock);
	inplaceReduceKernel<<<blocks, threadsPerBlock>>>(length, nlength,
						 scratchVecDevice);
	cudaThreadSynchronize();
    }
    cudaMemcpy(&val, &scratchVecDevice[0],
	       sizeof(float), cudaMemcpyDeviceToHost);
    return val;
}

Understanding this code

Note: reduceThreadsPerBlock and reduceDegree are parameters set by
   commandline options.  Typical values are 1024 and 2, respectively.

1. What is the importance of the first CudaMemcpy operation?
   Create a non-destrctive copy. Because we need run several times.
2. Macro UPDIV is defined as:
   Make sure there are enough block running.

#define UPDIV(n, d) (((n)+(d)-1)/(d))

   What math function does this compute?

3. Assume initial length = 8192, threadsPerBlock = 1024, and degree = 2.

   a. For the first iteration, what would be the values of length,
      nlength and blocks?

   b. What about for subsequent iterations?

   c. How many total kernel launches would there be?

   d. What's the purpose of the second cudaMemcpy operation

Measuring this code.

Compile the provided code.  Run it for different degrees
(selected via commandline option -d)

a. What degree gives the best performance
b. How many GigaFLOPS it hit?
c. Does that seem like a good number?

IV. Tweaking the CUDA Implementation

To get ready for the next step, it would be nice to have a version
that doesn't modify the source data.  Here's the start of a kernel:

// CUDA kernel
// Reduce src from length to nlength without altering source
__global__ void
copyReduceKernel(int length, int nlength, float *src, float *dest) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < nlength) {
    // Accumulate values for dest[idx]

    // Your code here

    }
}

Use inplaceReduceKernel as a guide for filling in the code.  (This is
a very simple exercise!)

Using this kernel is a bit tricky, since we need to provide the
program with scratch memory.

Here's one way to implement it:

// Reducer based on copying reductions
float
copyReduce(int length, float *srcVecDevice, float *scratchVecDevice) {
    int threadsPerBlock = reduceThreadsPerBlock;
    int degree = reduceDegree;
    float val;
    int sindex = 0;
    int nlength;
    float *src = srcVecDevice;
    float *dest;
    for (; length > 1; length = nlength) {
	nlength = UPDIV(length, degree);
	dest = &scratchVecDevice[sindex];
	int blocks = UPDIV(nlength, threadsPerBlock);
	copyReduceKernel<<<blocks, threadsPerBlock>>>(length, nlength,
						      src, dest);
	sindex += nlength;
	src = dest;
	cudaThreadSynchronize();
    }
    cudaMemcpy(&val, &dest[0], sizeof(float), cudaMemcpyDeviceToHost);
    return val;
}

Understanding this code:

a. What's the role of the variable sindex?

b. How will it change as the program executes?

c. Suppose the initial value of length is 513 and the degree is 2.
   What will be the successive values of length?

d. Give an upper bound on how much scratch memory this function requires.

e. Can you think of a way to manage the scratch memory that reduces the
   amount required?

Measuring this code:

Once you've fixed up the kernel, modify the macro REDUCER to be
copyReduce.  Compile and run the program for different values of
degree.  Is there any performance difference from before?

III. Intermediate CUDA Programming

Thus far, we have treated the GPU like an idealized SIMD machine.
Unfortunately, getting good performance often requires going further,
understanding the block structure of the thread processors.

CUDA programs can run much faster if you take advantage of the
machine's hierarchy.  In particular, within a single block you can
take advantage of:

* Fast shared memory
* Low-overhead synchronization.

Let's use these capabilities to write a reduction kernel that will do
a 1024:1 reduction within a single kernel execution.  Here's a
somewhat inefficient version:

// CUDA kernel
// Reduce portion of vector by threadsPerBlock
__global__ void
blockReduceKernel(int length, int degree, float *src, float *dest) {
    int idx = threadIdx.x;  // Thread ID within block
    int tidx = blockIdx.x;   // Identifies which tree this is part of
    int bsize = blockDim.x;
    int gidx = idx + blockDim.x * tidx;

    __shared__ float scratch[THREADSPERBLOCK];
    // First bring chunk of source array into shared memory
    float val = 0.0;

    // loading in parallel
    if (gidx < length)
	val += src[gidx];

    scratch[idx] = val;
	    
    __syncthreads();

    int nlen;
    for (int len = bsize; len > 1; len = nlen) {
	nlen = UPDIV(len, degree);
	if (idx < nlen) {
	    float val = scratch[idx];
	    for (int i = idx + nlen; i < len; i += nlen)
		val += scratch[i];
	    scratch[idx] = val;
	}
	__syncthreads();
    }
    if (idx == 0)
    dest[tidx] = scratch[0];
}

Understanding this code:

1. The code will be called with parameters length, src, and dest
   based on the full vector being reduced
   
2. The x dimension of the block parameters will have:
   - blockDim.x equal to the number of threads/block (call this T)
   - threadId.x varying between 0 and the T-1
   - blockId.x identifying which block this is among those that were
     launched

Questions:

   - If length = N, how many blocks will be generated by the launch?
   - What does the value "gidx" represent?

3. How is the value of the shared memory initialized?

4. What does __syncthreads() do?

5. How does the loop compare to the reduction code used for inplace
   reduction?

6. What is the effect of the final write?  How many threads within the
   block perform this write?

7. How well will this code utilize the processors?
   - During the initial read
   - On each iteration

Measuring this code

Set the macro REDUCER to blockReduce in reduce.cu.  Compile and run
the code for different degrees.  What is the peak performance?


Better Implementation

Finally, let's look at a version that does an initial level of
reduction while reading data into scratch memory.  Here's the general
form of the code:

// Reduce portion of vector by degree * threadsPerBlock
__global__ void
blockCollectReduceKernel(int length, int threads, int degree,
			 float *src, float *dest) {
    int idx = threadIdx.x;  // Thread ID within block
    int tidx = blockIdx.x;   // Identifies which tree this is part of
    int bsize = blockDim.x;
    int gidx = idx + blockDim.x * tidx;

    __shared__ float scratch[THREADSPERBLOCK];

    // First pass sums source values into shared memory
    float val = 0.0;

    // Accumulate values for scratch[idx]
    // by indexing from gidx up to length
    // with a stride of threads

    // Your code here


    scratch[idx] = val;
    __syncthreads();

    int nlen;
    for (int len = bsize; len > 1; len = nlen) {
	nlen = UPDIV(len, degree);
	if (idx < nlen) {
	    float val = scratch[idx];
	    for (int i = idx + nlen; i < len; i += nlen)
		val += scratch[i];
	    scratch[idx] = val;
	}
	__syncthreads();
    }

    if (idx == 0)
	dest[tidx] = scratch[0];
}

Understanding this code:

1. This code performs a reduction by a factor of D*T, where D = degree
   and T = the number of threads per block.

2. It gets called with:
   - B = number of blocks = UPDIV(length, D*T)
   - threads = B * T

Question:
   - What is the significance of threads?
   
Implementing the first-level of reduction.

Your job is to fill in the code performing the first level of
reduction, where each thread reads up to D values from src.  Imagine a
strided access pattern.

- What will the stride size be?  Why?
- What will the first value be?
- What will be the stopping point?

Measuring this code

After writing this loop, change the macro REDUCER to
blockCollectReduce.  Compile and run the code for different values of
degree.  How fast does it run?  What factors are limiting performance?


